{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Spark\n",
        "\n",
        "Run the following code to install Spark in your local environment (this must be done only once)\n",
        "\n",
        "1. **IMPORTANT**: Java JDK (version 8 or higher) must be installed in your machine.\n",
        "2. Download Spark at [this link](https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz).\n",
        "3. Run the next cell by changing the path to where you have downloaded the file above."
      ],
      "metadata": {
        "id": "EsElqAaj4Sse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbvEUbWIHm2s"
      },
      "outputs": [],
      "source": [
        "!tar xf \"C:\\\\Users\\\\Enrico\\\\Downloads\\\\spark-3.5.1-bin-hadoop3.tgz\"\n",
        "!pip install -q findspark"
      ]
    },
    {
      "source": [
        "# Initialize Spark\n",
        "\n",
        "Check the comments to change the paths to your Java and Spark homes (this must be done every time you run the notebook)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_231\" # Change this to the path of your JDK folder\n",
        "os.environ[\"SPARK_HOME\"] = \"C:\\\\Users\\\\Enrico\\\\Downloads\\\\spark-3.5.1-bin-hadoop3\" # Change this to the directory where you have extracted the .tgz file by running the command in the above cell\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find() # Should return the directory of the Spark home"
      ],
      "metadata": {
        "id": "4oTFM5YtJvv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "sc"
      ],
      "metadata": {
        "id": "KJlzVAmbJ9vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark: working with RDDs\n",
        "\n",
        "Check the documentation: [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis)."
      ],
      "metadata": {
        "id": "oBd7XwkFBDEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basics"
      ],
      "metadata": {
        "id": "jcoFwGvm4gj6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "vlFswJyWytKG"
      },
      "outputs": [],
      "source": [
        "# let's create a simple example\n",
        "riddle1 = \"over the bench the sheep lives under the bench the sheep dies\"\n",
        "riddle2 = [\"over the bench the sheep lives\", \"under the bench the sheep dies\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "WHywj4BmytKH"
      },
      "outputs": [],
      "source": [
        "# create an RDD from the `riddle` string\n",
        "rdd1 = sc.parallelize(riddle1.split(\" \"))\n",
        "# each tuple of the RDD corresponds to a single word\n",
        "\n",
        "print(rdd1)\n",
        "# why is there no result returned?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "3oYjBLnOytKI"
      },
      "outputs": [],
      "source": [
        "# compute the RDD\n",
        "print(rdd1.collect())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = sc.parallelize(riddle2)\n",
        "print(rdd2.collect())"
      ],
      "metadata": {
        "id": "exzQLruZ9qgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations"
      ],
      "metadata": {
        "id": "iyH7halU9HiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# map: returns a new RDD by applying a function to each of the elements in the original RDD\n",
        "rdd1.map(lambda s: s.upper()).collect()"
      ],
      "metadata": {
        "id": "sndBHyEF86T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flatMap: returns a new RDD by applying the function to every element of the parent RDD and then flattening the result\n",
        "rdd2.flatMap(lambda s: s.split(\" \")).collect()"
      ],
      "metadata": {
        "id": "9MlPRBd_-Cl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter: returns a new RDD containing only the elements in the parent RDD that satisfy the function inside filter\n",
        "rdd1.filter(lambda s: s.startswith(\"u\")).collect()"
      ],
      "metadata": {
        "id": "UlT_jxmH9Myx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distinct: returns a new RDD that contains only the distinct elements in the parent RDD\n",
        "rdd1.distinct().collect()"
      ],
      "metadata": {
        "id": "QxxJdRxW-Xcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# groupByKey: groups the values for each key in the (key, value) pairs of the RDD into a single sequence\n",
        "rdd1.map(lambda s: (s,1)).groupByKey().mapValues(list).collect()\n",
        "\n",
        "# (first map converts to a key-value RDD)\n",
        "# (mapValues is a map that operates only on the values - in this case, used to convert from ResultIterable to List for printing reasons)"
      ],
      "metadata": {
        "id": "dBAh2Gs8-fdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduceByKey: when called on a key-value RDD, returns a new dataset in which the values for each of its key are aggregated\n",
        "rdd1.map(lambda s: (s,1)).reduceByKey(lambda x, y: x + y).collect()"
      ],
      "metadata": {
        "id": "cH1dxiGl_cS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sortByKey: returns a new RDD with (key,value) pairs of parent RDD in sorted order according to the key\n",
        "rdd1.map(lambda s: (s,1)).sortByKey().collect()"
      ],
      "metadata": {
        "id": "-XJWbbv6_0Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join: starting from two RDD with (key, value1) and (key, value2) pairs, returns a new RDD with (key, (value1, value2)) pairs\n",
        "\n",
        "# rddProvinces: (initials, name, region)\n",
        "rddProvinces = sc.parallelize([(\"BO\", \"Bologna\", \"Emilia-Romagna\"),(\"RA\", \"Ravenna\", \"Emilia-Romagna\"),(\"MI\", \"Milan\", \"Lombardia\")])\n",
        "# rddPeople: (id, name, province)\n",
        "rddPeople = sc.parallelize([(1, \"Enrico\", \"RA\"),(2, \"Alice\", \"RA\"),(3, \"Bob\", \"BO\"),(4, \"Charlie\", \"FC\")])\n",
        "\n",
        "# This does not work\n",
        "rddPeople.join(rddProvinces).collect()"
      ],
      "metadata": {
        "id": "SGIn90U0Md8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rddProvinces2 = rddProvinces.map(lambda p: (p[0], (p[1],p[2])))\n",
        "rddProvinces2.collect()"
      ],
      "metadata": {
        "id": "gQ54N0nKjmJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rddPeople2 = rddPeople.map(lambda p: (p[2], (p[0],p[1])))\n",
        "rddPeople2.collect()"
      ],
      "metadata": {
        "id": "PmG1337hk0a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rddPeople2.join(rddProvinces2).collect()"
      ],
      "metadata": {
        "id": "AJaZHHXYj-vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actions"
      ],
      "metadata": {
        "id": "gKJ_7MCsAIy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect: returns a list that contains all the elements of the RDD\n",
        "rdd1.collect()"
      ],
      "metadata": {
        "id": "XQXX-d20_vWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count: returns the number of elements in the RDD\n",
        "rdd1.count()"
      ],
      "metadata": {
        "id": "FXuL9K2qATfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce: aggregates the elements of the RDD using a function that takes two elements of the RDD as input and gives the result\n",
        "sc.parallelize([1, 2, 3, 4, 5]).reduce( lambda x, y: x * y)"
      ],
      "metadata": {
        "id": "YI10eDDnAWe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take: returns the first n elements of RDD in the same order\n",
        "rdd1.take(2)"
      ],
      "metadata": {
        "id": "CgN5qtwBAvIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saveAsTextFile: saves the content of the RDD to a file\n",
        "rdd1.saveAsTextFile(\"rdd1\")"
      ],
      "metadata": {
        "id": "yS8SuYRQvV6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples"
      ],
      "metadata": {
        "id": "gmosJNIOA5S_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "mGRkO7_oytKK"
      },
      "outputs": [],
      "source": [
        "# Flatten the words beginning with the letter C\n",
        "# - map: transform each string in upper case (remember: map returns a new RDD with the same cardinality)\n",
        "# - filter: keep only the strings beginning with \"C\" (remember: filter returns a new RDD with the same or smaller cardinality)\n",
        "# - flatMap: explode each string into its characters (remember: flatMap returns a new RDD with the any cardinality)\n",
        "rdd1\\\n",
        "    .map(lambda s: s.upper())\\\n",
        "    .filter(lambda s: s.startswith(\"U\"))\\\n",
        "    .flatMap(lambda s: list(s))\\\n",
        "    .collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "Wrr37cZ2ytKL"
      },
      "outputs": [],
      "source": [
        "# A simple word count\n",
        "# - map: map each word to a tuple (word, 1); each tuple represent the count associate with a word\n",
        "# - reduceByKey: group all the tuples with the same word and sum the counts\n",
        "# - sortBy: sort tuples by count\n",
        "rdd1\\\n",
        "    .map(lambda s: (s, 1))\\\n",
        "    .reduceByKey(lambda a, b: a + b)\\\n",
        "    .sortBy(lambda x: x[1], False)\\\n",
        "    .collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute average length of words depending on their initial letter\n",
        "# map: map each word to a key-value tuple (word, (wordLength, 1)), where the value is an object composed by two value: the length of the word and a 1\n",
        "# reduceByKey: group all the tuples with the same word to 1) sum the lengths, and 2) sum the counts\n",
        "# mapValues: divides the sums by the counts to compute the averages\n",
        "# sortBy: sort tuples by averages\n",
        "rdd1\\\n",
        "  .map(lambda s: (s[0], (len(s),1)))\\\n",
        "  .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
        "  .mapValues(lambda x: x[0]/x[1])\\\n",
        "  .sortBy(lambda x: x[1], False)\\\n",
        "  .collect()"
      ],
      "metadata": {
        "id": "zxBoMsnTCATh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark: working with DataFrames\n",
        "\n",
        "Check the documentation: [here](https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.sql.DataFrame.html).\n",
        "\n",
        "What is different from Pandas' DataFrames?\n",
        "\n",
        "- Spark supports parallelization (Pandas doesn't), thus it's more suitable for big data processing\n",
        "- Spark follows Lazy Execution, which means that a task is not executed until an action is performed (Pandas follows Eager Execution, which means task is executed immediately)\n",
        "- Spark has immutability (Pandas has mutability)\n",
        "- The data structure is similar, the APIs are different"
      ],
      "metadata": {
        "id": "sCXrRXAAEtet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(\"../datasets/housing.csv\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "IGChYOQWRZku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switching from Spark to Pandas\n",
        "pandasDF = df.toPandas()\n",
        "print(pandasDF)"
      ],
      "metadata": {
        "id": "_aSykf1wV8dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switching from Pandas to Spark\n",
        "df = spark.createDataFrame(pandasDF)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "cFUtMUaUWTY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select: returns a new DataFrame with only selected columns (similar to a map on RDDs)\n",
        "df.select('population','median_house_value').show()"
      ],
      "metadata": {
        "id": "eFAs2zBHPLTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select, similarly to a map, allows column values to be redefined\n",
        "df.select(df.population,df.median_house_value/1000).show()\n",
        "# put the operation within parenthesis and add .alias('median_house_value_in_K$')"
      ],
      "metadata": {
        "id": "FmClDtFaQqO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# withColumn: used to manipulate (rename, change the value, convert the datatype)\n",
        "# an existing column in a dataframe (or to create a new column) while keeping the rest intact\n",
        "df.withColumn('median_house_value_in_K$',df.median_house_value/1000).show()"
      ],
      "metadata": {
        "id": "4whu8BylWfAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter: returns a new DataFrame containing only the elements in the parent DataFrame that satisfy the function inside filter (as in RDDs)\n",
        "# orderBY: orders the DataFrame by the selected column(s)\n",
        "df.filter(df.population > 1000).orderBy(df.population.asc()).show()"
      ],
      "metadata": {
        "id": "9zrnJn3_Q0PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# groupBy: returns a new DataFrame which is the result of an aggregation\n",
        "df.groupBy(df.ocean_proximity).agg({'median_house_value': 'avg', '*': 'count'}).show()"
      ],
      "metadata": {
        "id": "1qv5_B90Raho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# withColumnRenamed: rename a column\n",
        "df.groupBy(df.ocean_proximity).agg({'*': 'count'}).withColumnRenamed(\"count(1)\", \"tot\").show()"
      ],
      "metadata": {
        "id": "E3b1rxzMN7Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL queries can be run on a DataFrame\n",
        "df.createOrReplaceTempView(\"housing\")\n",
        "spark.sql(\"select ocean_proximity, avg(median_house_value) as avg_price from housing group by ocean_proximity order by avg_price desc\").show()"
      ],
      "metadata": {
        "id": "TaP-GatdTD7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: creating a cube\n",
        "\n",
        "You are working with two files:\n",
        "\n",
        "- weather-stations.csv: it contains a list of weather stations that capture weather information every day of every year throughout the world\n",
        "  - Each station is identified by a StationID\n",
        "- weather-sample-10k.csv: it contains the data measured by a certain station on a certain date (a sample of 10k lines collected from the National Climatic Data Center of the USA)\n",
        "  - Each weather measurenent is identified by a StationID and a Timestamp\n",
        "\n",
        "Your goal is to create a single file representing the following cube and to run some queries through PowerBI.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "The procedure to create the cube is the following.\n",
        "\n",
        "1. On the stations file:\n",
        "  1. replace empty states and countries in stations with a placeholder value (e.g., \"XX\");\n",
        "  1. keep only the following fields: stationId, state, country\n",
        "2. On the weather-sample file:\n",
        "  1. filter out weather wrong measurements (i.e., where airTemperatureQuality=9);\n",
        "  1. keep only the following fields: stationId, airTemperature, date, month, year\n",
        "  1. create a new fulldate field by concatenating year, month, and date\n",
        "  1. create a new fullmonth field by concatenating year and month\n",
        "1. Join stations with weather measurements on the stationId field\n",
        "1. Keep only the following fields: state, country, fulldate, fullmonth, year, airTemperature\n",
        "1. Aggregate the measurements by state and date to take the average temperature\n",
        "  - Group by: state, country, fulldate, fullmonth, year\n",
        "  - Calculation: avg(airTemperature)\n",
        "1. Save the result on a file"
      ],
      "metadata": {
        "id": "Dv5BUnaAWWEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark"
      ],
      "metadata": {
        "id": "xO6bQ3NQwDAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfW = sc.textFile(\"../datasets/weather-sample-10k.txt\")\\\n",
        "  .map(lambda l: (l[4:15],l[15:19],l[19:21],l[21:23],int(l[87:92])/10,l[92:93]))\\\n",
        "  .toDF([\"stationId\",\"year\",\"month\",\"day\",\"airTemperature\",\"airTemperatureQuality\"])\n",
        "dfW.show()"
      ],
      "metadata": {
        "id": "Xcd4fAhgeDhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat\n",
        "dfS = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").csv(\"../datasets/weather-stations.csv\")\n",
        "dfS = dfS.select(concat(dfS[0],dfS[1]),dfS[2],dfS[3],dfS[4],dfS[5],dfS[6],dfS[7],dfS[8],dfS[9],dfS[10])\\\n",
        "  .toDF(\"stationId\",\"city\",\"country\",\"state\",\"call\",\"latitude\",\"longitude\",\"elevation\",\"date_begin\",\"date_end\")\n",
        "dfS.show()"
      ],
      "metadata": {
        "id": "OaknyiXHeCbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. On the stations file:\n",
        "  1. replace empty states and countries in stations with a placeholder value (e.g., \"XX\");\n",
        "  1. keep only the following fields: stationId, state, country"
      ],
      "metadata": {
        "id": "8QqJZrglm-ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfS1 = dfS.fillna({'state': 'XX', 'country':'XX'})\n",
        "dfS2 = dfS1.select('stationId','state','country')\n",
        "dfS2.show()"
      ],
      "metadata": {
        "id": "8v1YuN-4nQCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. On the weather-sample file:\n",
        "  1. filter out weather wrong measurements (i.e., where airTemperatureQuality=9);\n",
        "  1. keep only the following fields: stationId, airTemperature, date, month, year\n",
        "  1. create a new fulldate field by concatenating year, month, and date\n",
        "  1. create a new fullmonth field by concatenating year and month"
      ],
      "metadata": {
        "id": "5LxL1WQvnF0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat, lit\n",
        "dfW1 = dfW.where(\"airTemperature < 9\")\n",
        "dfW2 = dfW1.select('stationId','airTemperature','day','month','year')\n",
        "dfW3 = dfW2.withColumn(\"fulldate\", concat(dfW1.year,lit(\"-\"),dfW1.month,lit(\"-\"),dfW1.day))\n",
        "dfW4 = dfW3.withColumn(\"fullmonth\", concat(dfW1.year,lit(\"-\"),dfW1.month))\n",
        "dfW4.show()"
      ],
      "metadata": {
        "id": "Cf9bm6CspR8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Join stations with weather measurements on the stationId field\n"
      ],
      "metadata": {
        "id": "jy2yp35unHhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfJ = dfS2.join(dfW4, \"stationId\")\n",
        "dfJ.show()"
      ],
      "metadata": {
        "id": "oHTKQhQJrPMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Keep only the following fields: state, country, fulldate, fullmonth, year, airTemperature"
      ],
      "metadata": {
        "id": "mv_dfniznJ2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfJ2 = dfJ.select(\"state\", \"country\", \"fulldate\", \"fullmonth\", \"year\", \"airTemperature\")\n",
        "dfJ2.show()"
      ],
      "metadata": {
        "id": "dYyMxZaSrprK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Aggregate the measurements by state, country and date to take the average temperature\n",
        "  - Group by: state, country, fulldate, fullmonth, year\n",
        "  - Calculation: avg(airTemperature)"
      ],
      "metadata": {
        "id": "YXKChhXwnLlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfG = dfJ2.groupBy(\"state\", \"country\", \"fulldate\", \"fullmonth\", \"year\").agg({'airTemperature': 'avg'})\n",
        "dfG.show()"
      ],
      "metadata": {
        "id": "shVr9k9hryqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Save the result on a file"
      ],
      "metadata": {
        "id": "zwXx8h-tv4-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfG.write.mode('overwrite').option('header','true').csv(\"weather-cube\")"
      ],
      "metadata": {
        "id": "pKTH93LQu61U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PowerBI\n",
        "\n",
        "Download the file from the left panel of this notebook (in case of issues, download it from [here](https://raw.githubusercontent.com/w4bo/2024-bbs-dm/master/materials/results/weather-cube.csv)) and load it in [Power BI](https://app.powerbi.com/).\n",
        "- Visualize the daily trend of average temperatures for each country\n",
        "- Show the average temperature on the map\n",
        "\n",
        "\n",
        "The final Power BI file will be available [here](https://raw.githubusercontent.com/w4bo/2024-bbs-dm/master/materials/results/weather-cube.pbix)."
      ],
      "metadata": {
        "id": "jkKWX-mWv8S9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional exercises\n",
        "\n",
        "The solution will be available [here](https://raw.githubusercontent.com/w4bo/2024-bbs-dm/master/materials/results/2024_bbs_dm_spark_basics_solution.ipynb).\n",
        "\n",
        "## Getting familiar with data frame transformations\n",
        "\n",
        "Carry out the following operations (in any order).\n",
        "\n",
        "- ```.select()``` operator; starting from ```dfS```:\n",
        "  1. keep only country, elevation, date_begin, and date_end\n",
        "  1. keep only the first four characters of date_begin using [sf.substring](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.substring.html)\n",
        "  1. use [concat](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.concat.html) to concatenate date_begin with date_end and putting an underscore (\\_) in the middle; since the underscore is not a column, declare it as ```sf.lit(\"_\")```\n",
        "  1. use [coalesce](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.coalesce.html) to take the value of country if not null, otherwise the value of city\n",
        "  1. put countries in lowercase using [sf.lower](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lower.html)\n",
        "  1. in the previous four points, use ```.alias()``` to give a meaningful name to the obtained columns\n",
        "- ```.withColumn()``` operator; starting from ```dfS```:\n",
        "  1. do the same as points 2 to 5 of the ```select``` operator, but using ```withColumn```\n",
        "- ```.filter()``` operator; starting from ```dfS```, keep only the rows where:\n",
        "  1. elevation is greater than 5000\n",
        "  1. country is not null, using [sf.isnotnull](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.isnotnull.html)\n",
        "  1. conditions 1 and 2 are both true; conditions must be put between parenthesis and separated by \"&\" (e.g., check [here](https://www.geeksforgeeks.org/pyspark-filter-dataframe-based-on-multiple-conditions/))\n",
        "  1. either one of conditions 1 and 2 is true; conditions must be put between parenthesis and separated by \"|\" (e.g., check [here](https://www.geeksforgeeks.org/pyspark-filter-dataframe-based-on-multiple-conditions/))\n",
        "  1. date_begin is the first day of the month (requires to use substring)\n",
        "- ```.groupBy()``` operator; starting from ```dfW```:\n",
        "  1. group by airTemperatureQuality to count how many rows there are for each value\n",
        "  1. as above, but also calculate the average temperature\n",
        "  1. as above, but also given meaningful names to the results using ```withColumnRenamed```\n",
        "  1. group by month to calculate the minimum and maximum temperatures and order by month using ```orderby```; to aggregate differently on the same column, use the [sf.max](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.max.html) and [sf.min](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.min.html) functions inside the ```agg``` function instead of the object enclosed by brackets ```{}```\n",
        "  1. group by month and day to calculate the minimum and maximum temperatures\n",
        "  1. group by stationId and year month to count the number of rows"
      ],
      "metadata": {
        "id": "679cmN5vLOMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as sf\n",
        "\n",
        "# SELECT\n",
        "\n",
        "# dfS.select(\"country\", \"elevation\", \"date_begin\", \"date_end\").show()\n",
        "# dfS.select(sf.substring(dfS.date_begin,0,4).alias(\"substringing\")).show()\n",
        "# dfS.select(concat(dfS.date_begin,sf.lit(\"_\"),dfS.date_end).alias(\"concatenating\")).show()\n",
        "# dfS.select(sf.coalesce(dfS.country, dfS.city).alias(\"coalescing\")).show()\n",
        "# dfS.select(sf.lower(dfS.country).alias(\"lowercasing\")).show()\n",
        "\n",
        "# WITH_COLUMN\n",
        "\n",
        "# dfS.withColumn(\"substringing\",sf.substring(dfS.date_begin,1,4)).show()\n",
        "# dfS.withColumn(\"concatenating\",concat(dfS.date_begin,sf.lit(\"_\"),dfS.date_end)).show()\n",
        "# dfS.withColumn(\"coalescing\",sf.coalesce(dfS.country, dfS.city)).show()\n",
        "# dfS.withColumn(\"lowercasing\",sf.lower(dfS.country)).show()\n",
        "\n",
        "# FILTER\n",
        "\n",
        "# dfS.filter(dfS.elevation>5000).show()\n",
        "# dfS.filter(sf.isnotnull(dfS.country)).show()\n",
        "# dfS.filter((sf.isnotnull(dfS.country)) & (dfS.elevation>5000)).show()\n",
        "# dfS.filter((sf.isnotnull(dfS.country)) | (dfS.elevation>5000)).show()\n",
        "# dfS.filter(sf.substring(dfS.date_begin,7,2)==\"01\").show()\n",
        "\n",
        "# GROUP BY\n",
        "\n",
        "# dfW.groupBy(dfW.airTemperatureQuality).agg({'*': 'count'}).show()\n",
        "# dfW.groupBy(dfW.airTemperatureQuality).agg({'*': 'count', 'airTemperature': 'avg'}).show()\n",
        "# dfW.groupBy(dfW.airTemperatureQuality).agg({'*': 'count', 'airTemperature': 'avg'}).withColumnRenamed('avg(airTemperature)','avg-temperature').withColumnRenamed('count(1)','cnt').show()\n",
        "# dfW.groupBy(dfW.month).agg(sf.min(dfW.airTemperature),sf.max(dfW.airTemperature)).orderBy(dfW.month).show()\n",
        "# dfW.groupBy(dfW.month,dfW.day).agg(sf.min(dfW.airTemperature),sf.max(dfW.airTemperature)).show()\n",
        "# dfW.groupBy(dfW.stationId,dfW.year).agg({'*': 'count'}).show()"
      ],
      "metadata": {
        "id": "82YodmWqG-JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete exercises\n",
        "\n",
        "Carry out the following exercises (in any order).\n",
        "\n",
        "- Check if there exist stations with a negative elevation; then, calculate how many of these stations exist in each country; rename the result to \"cnt\" and order the result by decreasing cnt\n",
        "- Take only stations with positive elevation, compute the maximum elevation by country and rename the result to \"elevation\"; then, join the result with the original dfS to get, for each country, the name of the city with the highest elevation (join key: ```[\"country\",\"elevation\"]```); order the result by decreasing elevation\n",
        "- Take only weather values with airQuality==1, compute the minimum temperature for each stationId and rename it to \"minTemperature\"; then, join the result with dfS and keep only the columns \"minTemperature\" and \"elevation\"; finally, use the correlation between the two columns. To do the last part, you need to:\n",
        "  - cast the elevation to an integer datatype: you need to add ```from pyspark.sql.types import IntegerType``` and then ```df.myfield.cast(IntegerType())```;\n",
        "  - compute the correlation with ```df.stat.corr(\"myfield1\",\"myfield2\")```."
      ],
      "metadata": {
        "id": "JFGe5AvcHCKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfS3 = dfS.filter(dfS.elevation<0).groupBy(dfS.country).agg({'*': 'count'}).withColumnRenamed('count(1)','cnt')\n",
        "dfS3.orderBy(dfS3.cnt.desc()).show()"
      ],
      "metadata": {
        "id": "AG9YwgeaL_Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfS4 = dfS.filter(dfS.elevation>=0).groupBy(dfS.country).agg({'elevation': 'max'}).withColumnRenamed('max(elevation)','elevation')\n",
        "dfS4.join(dfS,[\"country\",\"elevation\"]).orderBy(dfS4.elevation.desc()).show()"
      ],
      "metadata": {
        "id": "cxgGKur1Pwjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "dfW5 = dfW.filter(dfW.airTemperatureQuality==1).groupBy(dfW.stationId).agg({'airTemperature': 'min'}).withColumnRenamed('min(airTemperature)','minTemperature')\n",
        "dfW6 = dfW5.join(dfS,\"stationId\").select(\"minTemperature\",\"elevation\")\n",
        "dfW7 = dfW6.withColumn(\"elevation\",dfW6.elevation.cast(IntegerType()))\n",
        "dfW7.stat.corr(\"minTemperature\",\"elevation\")\n",
        "dfW7.show()"
      ],
      "metadata": {
        "id": "50-P6zgITVn_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}